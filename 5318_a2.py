# -*- coding: utf-8 -*-
"""5318-a2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tNxNwpei_c1IEFWwvpZ3gJQg-H4bPC5y
"""

import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.neighbors import KNeighborsClassifier
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import mutual_info_classif

import matplotlib.pyplot as plt

from sklearn.model_selection import GridSearchCV

from google.colab import drive
drive.mount('/content/drive')

from pickle import load
with open('/content/drive/MyDrive/Colab Notebooks/emnist_train.pkl', 'rb') as f:
    x = load(f)
with open('/content/drive/MyDrive/Colab Notebooks/emnist_test.pkl', 'rb') as f:
    test = load(f)

#data = 'data'
#target = 'labels'
#knn = KNeighborsClassifier(n_neighbors=9)
#knn.fit(x[data].reshape(100000, 28*28), x[target])
#knn_score = knn.score(test[data].reshape(20000, 28*28), test[target])

#print('KNN score:', knn_score)

# knn
# pre-processing or feature deduction
data = 'data'
target = 'labels'

X_train, X_val, y_train, y_val = train_test_split(x[data], x[target], test_size=0.2, random_state=42)

X_train_cnn = X_train.reshape(X_train.shape[0], 28, 28, 1) # / 255.0
X_val_cnn = X_val.reshape(X_val.shape[0], 28, 28, 1) # / 255.0

# Define CNN architecture
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(62, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train CNN
model.fit(X_train_cnn, tf.keras.utils.to_categorical(y_train), epochs=5, validation_data=(X_val_cnn, tf.keras.utils.to_categorical(y_val)))

# Extract features from the last layer
extractor = Sequential(model.layers[:-1])
X_train_features = extractor.predict(X_train_cnn)
X_val_features = extractor.predict(X_val_cnn)

# Select the most important features
feature_scores = mutual_info_classif(X_train_features, y_train)
selected_features = X_train_features[:, feature_scores > 0.05]
selected_val = X_val_features[:, feature_scores > 0.05]

X_train_cnn = X_train.reshape(X_train.shape[0], 28, 28, 1) # / 255.0
full_train_features = extractor.predict(X_train_cnn)

# Train KNN on the selected features
knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(selected_features, y_train)
knn_score = knn.score(X_val_features[:, feature_scores > 0.05], y_val)

print('KNN score with selected features:', knn_score)

# KNN
k = [3, 5, 7, 9] # n_neighbors 
p = [1, 2]

def bestKNNClassifier(X_train, X_test, y_train, y_test):
    # grid search
    param_KNN = {'n_neighbors': k,
                 'p': p}
    grid_KNN = GridSearchCV(KNeighborsClassifier(),
                            param_KNN, cv=5,
                            return_train_score=True) 
    grid_KNN.fit(X_train, y_train)
    
    # test and return
    test_set_score = grid_KNN.score(X_test, y_test)
    cross_validation_score = grid_KNN.best_score_
    best_k = grid_KNN.best_params_['n_neighbors']
    best_p = grid_KNN.best_params_['p']
    
    return best_k, best_p, cross_validation_score, test_set_score

bestKNNClassifier(selected_features, selected_val, y_train, y_val)